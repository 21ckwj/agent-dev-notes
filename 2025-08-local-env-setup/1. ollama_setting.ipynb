{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc29f4cf-9b6e-48fb-a1d5-a9cee634f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d622e8f3-9f98-4baf-806c-e0ace750744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3999118f-166a-441f-b0a2-b88739259366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1:8b',\n",
       " 'created_at': '2025-07-21T13:08:21.8715921Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': '네, Large Language Model(LLM)은 자연어 처리(NLP) 기술을 기반으로 한 인공지능입니다.\\n\\nLLM의 핵심은 수천만 개의 텍스트 데이터를 학습하여 언어 패턴과 의미를 분석하는 것입니다. 이는 딥 러닝 알고리즘을 사용해 모델이 입력된 문장을 이해하고 관련된 내용을 생성하도록 합니다.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 26303996200,\n",
       " 'load_duration': 11367095800,\n",
       " 'prompt_eval_count': 42,\n",
       " 'prompt_eval_duration': 3212280600,\n",
       " 'eval_count': 82,\n",
       " 'eval_duration': 11715771800}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ollama_chat(model=\"\", messages= []):\n",
    "\n",
    "    response =requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\"model\": model, \"messages\":messages, \"stream\":False},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "response = ollama_chat(\n",
    "    model=\"llama3.1:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"LLM은 어떤 원리로 작동하나요? 50자 이내로 설명해주세요.\"}\n",
    "    ]     \n",
    ")\n",
    "\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa6ba1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": \"한국의 수도는?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "res = requests.post(url, json=payload)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d58598",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://localhost:11434/api/generate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     response.raise_for_status()\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m response = \u001b[43mollama_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLLM은 어떤 원리로 작동하나요? 50자 이내로 설명해주세요.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mollama_generate\u001b[39m\u001b[34m(model, prompt)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mollama_generate\u001b[39m(model=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, prompt=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m     response = requests.post(\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:11434/api/generate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m         json={\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m         },\n\u001b[32m     11\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\21ckw\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: http://localhost:11434/api/generate"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ollama_generate(model=\"\", prompt=\"\"):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        },\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "response = ollama_generate(\n",
    "    model=\"llama3\",\n",
    "    prompt=\"LLM은 어떤 원리로 작동하나요? 50자 이내로 설명해주세요.\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0211885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI(Artificial Intelligence)는 인공 지능을 의미하며, 인간의 지능을 모방하여 컴퓨터나 로봇으로 구현한 지능입니다.\n",
      "\n",
      "AI는 다양한 분야에서 발전을 거치며, 현재도 빠르게 발전하고 있습니다. AI 기술은 머신 러닝, 딥 러닝, 자연어 처리,Computer Vision 등 여러 가지 기술로 구성되어 있으며, 각 기술의 특징과 응용 사례를 살펴보면 다음과 같습니다.\n",
      "\n",
      "1. **머신 러닝(Machine Learning)**: 컴퓨터가 데이터에 대한 학습을 통해 향상되는 지능을 가지는 알고리즘입니다.\n",
      "2. **딥 러닝(Deep Learning)**: 머신 러닝에서 사용하는 기술 중 하나로, 신경망 구조를 사용하여 데이터를 분석합니다.\n",
      "3. **자연어 처리(Natural Language Processing, NLP)**: 컴퓨터가 인간 언어를 이해하고 해석하는 기술입니다.\n",
      "4. **Computer Vision**: 컴퓨터가 화상이나 영상 데이터를 분석하고 인식하는 기술입니다.\n",
      "\n",
      "AI 기술은 다음과 같은 다양한 분야에 적용할 수 있습니다:\n",
      "\n",
      "1. **인공지능(AI)**: 로봇, 자율주행차 등\n",
      "2. **스마트 홈(Smart Home)**: 가정에서 자동화된 시스템을 구축하는 것\n",
      "3. **의료**: 진단, 치료, 예방 등\n",
      "4. **금융**: 거래, 분석, 예측 등\n",
      "\n",
      "AI 기술의 장점은 다음과 같습니다:\n",
      "\n",
      "1. **자동화**: 많은 업무를 자동으로 처리할 수 있습니다.\n",
      "2. **개인화**: 개인 맞춤형 서비스 제공이 가능합니다.\n",
      "3. **예측**: 데이터를 분석하여 미래의 결과를 예측할 수 있습니다.\n",
      "\n",
      "그러나 AI 기술도 다음과 같은 단점이 있습니다:\n",
      "\n",
      "1. **부족한 지능**: 현재 AI는 인간의 지능을 완벽하게 모방하는 것은 어렵습니다.\n",
      "2. **보안**: AI가 공격에 취약하거나 취향에 따라 보안 설정이 바뀔 수 있습니다.\n",
      "\n",
      "AI 기술은 빠르게 발전하고 있습니다. 그러나 AI를 개발, 적용할 때는 다음과 같은 점을 고려해야 합니다:\n",
      "\n",
      "1. **책임감**: AI가 인간의 삶에 미치는 영향을 고려해야 합니다.\n",
      "2. **안정성**: AI가 안정적이고 신뢰할 수 있는지 확인해야 합니다.\n",
      "\n",
      "AI 기술은 많은 분야에서 혁신과 창조성을 가져올 것입니다. 따라서 AI를 이해하고 개발하는 능력을 키우는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1:8b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"AI Agent에 대해 설명해주세요\"}\n",
    "    ],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json()[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91a47ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1:8b',\n",
       " 'created_at': '2025-07-21T12:17:26.757578Z',\n",
       " 'message': {'role': 'assistant', 'content': '서울입니다.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 13411216300,\n",
       " 'load_duration': 11013345800,\n",
       " 'prompt_eval_count': 18,\n",
       " 'prompt_eval_duration': 1738750100,\n",
       " 'eval_count': 4,\n",
       " 'eval_duration': 656717900}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea931ed",
   "metadata": {},
   "source": [
    "# langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0100dc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280698e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
